{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Techniques using KerasNLP\n",
        "\n",
        "**Author:** [Usha Rengaraju](https://www.linkedin.com/in/usha-rengaraju-b570b7a2/)<br>\n",
        "**Date created:** 2023/07/10<br>\n",
        "**Last modified:** 2023/07/10<br>\n",
        "**Description:** Discussion of various sampling techniques using KerasNLP"
      ],
      "metadata": {
        "id": "BCM7_gNkHQCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "KerasNLP offers a wide range of sampling techniques already implemented and very easy to use. Samplers are basically the methods of selecting the next token based on the distribution of the previous tokens. There are various sampling techniques like `Beam Sampler` , `Contrastive Sampler`, `Random Sampler` and many more.\n",
        "\n",
        "This guide will show you how to use the various samplers and will also show the various hyperparameters of these samplers and how the output will be affected by these.\n"
      ],
      "metadata": {
        "id": "2TuJ9kYp1h73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & setup\n",
        "\n",
        "This tutorial requires you to have KerasNLP installed:\n",
        "\n",
        "```shell\n",
        "pip install keras-nlp\n",
        "```\n",
        "\n",
        "We begin by importing all required packages:"
      ],
      "metadata": {
        "id": "dSHlrOzp34QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading\n",
        "\n",
        "For this tutorial we will be using a pretrained `OPT Language Model` which is also directed loaded from the kerasnlp.\n",
        "\n",
        "For the output generation we take the sample input as `I recently ate`."
      ],
      "metadata": {
        "id": "eZrk35ou4EnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "\n",
        "opt_lm = keras_nlp.models.OPTCausalLM.from_preset(\"opt_1.3b_en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OERBsBGQAjY7",
        "outputId": "de72c9be-7978-4691-eb4a-599aba47da1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_1.3b_en/v1/vocab.json\n",
            "898822/898822 [==============================] - 1s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_1.3b_en/v1/merges.txt\n",
            "456318/456318 [==============================] - 1s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/opt_1.3b_en/v1/model.h5\n",
            "5263448608/5263448608 [==============================] - 265s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'embeddings/token_embedding1/embeddings:0' shape=(50272, 2048) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L6RETistBCt8",
        "outputId": "5ef11f75-9da7-470c-b476-24e7deb89932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate some raw mango and I had to stop because I was getting so sick. I had to go to the bathroom so much. It was awful. It was so bad, I was throwing up. I'm glad I was alone because\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam Sampler\n",
        "\n",
        "At each time-step, beam search keeps the beams (sequences) of the top num_beams highest accumulated probabilities, and uses each one of the beams to predict candidate next tokens."
      ],
      "metadata": {
        "id": "C6BwnjyGCFsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setting num_beams=2\n"
      ],
      "metadata": {
        "id": "gjeVPlnzEVvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=2))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DjtH0lohBbWw",
        "outputId": "e4461507-2202-410b-fea1-a201778162a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate at a restaurant that had a sign on the wall that said \"no dogs allowed\".   I asked the waitress if I could bring my dog. She said no. I asked if I could bring my cat. She said no'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### num_beams=10"
      ],
      "metadata": {
        "id": "MEX_nso2EcQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=10))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VqQxO0JPEgF-",
        "outputId": "360f0964-cf3c-4eb6-f34d-b0414d2f06ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate there for the first time. It was pretty good, but I don't think I'll be going back anytime soon. The service was terrible, the food was mediocre at best, and the drinks were overpriced.   I\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### num_beams=100\n"
      ],
      "metadata": {
        "id": "X3MvtknbExzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=100))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8YNj2QITE1rU",
        "outputId": "051815d7-7cea-42da-8f65-fbc74253fa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate there for the first time, and I have to say it was one of the best meals I've had in a long time.                    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Greedy Sampler\n",
        "\n",
        "This sampler is implemented on greedy search, i.e., always picking up the token of the largest probability as the next token."
      ],
      "metadata": {
        "id": "F-oIiT9TCALr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.GreedySampler())\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "id": "LvRmq2W0B_5T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "21e72978-e3e3-4ebd-c54c-8806f2165418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <bound method OPTCausalLM.generate_step of <keras_nlp.src.models.opt.opt_causal_lm.OPTCausalLM object at 0x7f07307369b0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate a whole bag of these. I was so disappointed.\\nI'm sorry to hear that. I've had them a few times and they're pretty good.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contrastive Sampler\n",
        "\n",
        "This sampler implements contrastive search algorithm. In short, the sampler chooses the token having the max \"score\" as the next token. The \"score\" is a weighted sum between token's probability and max similarity against previous tokens. By using this joint score, contrastive sampler reduces the behavior of duplicating seen tokens.\n",
        "\n",
        "**k**: int, the k value of top-k. Next token will be chosen from k tokens.<br>\n",
        "**alpha**: float, the weight of minus max similarity in joint score computation. The larger the value of alpha, the score relies more on the similarity than the token probability.<br>\n",
        "**seed**: int, defaults to None. The random seed."
      ],
      "metadata": {
        "id": "4vNSBq3PFi1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large alpha small k"
      ],
      "metadata": {
        "id": "5NaJgT4_GWRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=2, alpha=0.9, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ReIbegUtF9di",
        "outputId": "3babe00a-9eff-4645-dcbe-43673bd8667f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <bound method OPTCausalLM.generate_step of <keras_nlp.src.models.opt.opt_causal_lm.OPTCausalLM object at 0x7f07307369b0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate at a restaurant that had an all you can eat buffet. I was the first person in line and the server asked if I wanted to try anything. I said no, but I was curious about what they had to offer. She'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large alpha large k"
      ],
      "metadata": {
        "id": "d1CM3M7IKdzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=10, alpha=0.9, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "id": "2TndHwI9Kdz4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c13859e3-f9af-4fd3-8b04-42318d432af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate my weight (well a ton). It‼\\u200d♀️\\nCongo is proud to receive such well thought food. You should feel better for that, atleaet :)) Have my uptoke to counter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small alpha large k"
      ],
      "metadata": {
        "id": "FYkyjAsKKeol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=2, alpha=0.1, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "id": "_xCPUJcjKeom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6207ddc-8de2-417c-d868-7ac49ce0c03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate a whole bag of Doritos and a box of Cheetos. I'm not sure if I should be proud or ashamed.\\nI'm proud of you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small alpha small k"
      ],
      "metadata": {
        "id": "x2XPt99sGfvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=2, alpha=0.1, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BC62PHvRGjba",
        "outputId": "5fefd24f-1d71-4ac8-c667-46ce33239e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate a whole bag of Doritos and a box of Cheetos. I'm not sure if I should be proud or ashamed.\\nI'm proud of you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large k"
      ],
      "metadata": {
        "id": "GpLVHD21Gsya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=20, alpha=0.6, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fvTdI7nMGr-7",
        "outputId": "09b1eae0-dd37-4d65-e9bf-328afbe6a45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate at Pangea Seafood (they’re on Stoughton). Service was great, I didn’t feel like I was intruding on their day to day so I sat next to the cashier and ch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small k\n"
      ],
      "metadata": {
        "id": "UHHl1H8oGybM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.ContrastiveSampler(k=2, alpha=0.6, seed=45))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gmV3tmcmG0XN",
        "outputId": "529eef03-24e0-457e-a5b6-11bae4e28b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate at a restaurant that had a \"sausage and egg\" sandwich. It was a sausage, egg, and cheese sandwich with a side of bacon. I don\\'t know what the hell they were thinking, but it was delicious'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Sampler\n",
        "\n",
        "This sampler implements random sampling. Briefly, random sampler randomly selects a token from the entire distribution of the tokens, with selection chance determined by the probability of each token.\n",
        "\n",
        "**seed**: int, defaults to None. The random seed."
      ],
      "metadata": {
        "id": "qOSktCBDCQX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.RandomSampler(seed=51))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kagI2EvoCR92",
        "outputId": "bc8e9c7a-1a18-4add-c116-5f374be4215c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate chiethaya soup. My whole face seemed to have watery eyes for like the whole day. If you're in Bangkok I had no experience with it, it's one of the pearl shops and you try 3 items before purchasing\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top K Sampler\n",
        "\n",
        "This sampler implements top-k search algorithm. Briefly, top-k algorithm randomly selects a token from the tokens of top K probability, with selection chance determined by the probability.\n",
        "\n",
        "**k**: int, the k value of top-k. Next token will be chosen from k tokens.<br>\n",
        "**seed**: int, defaults to None. The random seed."
      ],
      "metadata": {
        "id": "7PVddlcyHQ3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k=2"
      ],
      "metadata": {
        "id": "wGdhUfIPHocx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopKSampler(k=5, seed=30))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uz-8Kg5mHn7i",
        "outputId": "6e0264db-f611-4dcf-a0df-717aa5d98b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate an egg that had been sitting out for a week, it wasn't as bad as the first egg I ate but still gross.\\nI think that's just the first one.\\nI don't even know what to think anymore\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k=10"
      ],
      "metadata": {
        "id": "v-c-A3wVHxSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopKSampler(k=10, seed=30))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FKZIFx2IHxSe",
        "outputId": "2baf255f-3cb5-43ce-cebe-d84fa466da19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate an entire can of tuna with some chips. I\\'m a pretty skinny dude, but this was a new level of \"I\\'m not fat.\"\\nI\\'ve been eating tuna for breakfast for several months now, because it\\'s easy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k=100"
      ],
      "metadata": {
        "id": "Awi8SxSoHxc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopKSampler(k=100, seed=30))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rqrp5vjYHxc8",
        "outputId": "5c20dd62-7766-4b50-f3fb-435f67be633b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate my first chicken sandwich from Piedmont Fresh Market. Not a huge fan because the tomatoes are a bit watery and the breading isn't very chewy. The tomatoes made the sandwich rather sweet and sour.   I\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-P Sampler\n",
        "\n",
        "This sampler implements top-p search algorithm. Top-p search selects tokens from the smallest subset of output probabilities that sum to greater than p. Put in another way, top-p will first order token predictions by likelihood, and ignore all tokens after the cumulative probability of selected tokens exceeds p, then select a token from the remaining tokens.\n",
        "<br>\n",
        "<br>\n",
        "**p**: float, the p value of top-p.<br>\n",
        "**k**: int, defaults to None. If set, this argument defines a heuristic \"top-k\" cutoff applied before the \"top-p\" sampling. All logits not in the top k will be discarded, and the remaining logits will be sorted to find a cutoff point for p. Setting this arg can significantly speed sampling up by reducing the number of tokens to sort.<br>\n",
        "**seed**: int, defaults to None. The random seed."
      ],
      "metadata": {
        "id": "LddHGF2mIQY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low p and no top-k"
      ],
      "metadata": {
        "id": "1bNAWmpQI6mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopPSampler(p=0.1, k=None, seed=22))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "coecTUKWIZ43",
        "outputId": "bb4f09e9-7e92-431a-c242-087827ec4921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate a lot of meat and I'm not sure if I'm going to be able to stop. I'm not sure if I'm going to be able to stop.\\nI'm not sure if I'm going to be able to\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low p and top-k"
      ],
      "metadata": {
        "id": "4JLqJjskI-lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopPSampler(p=0.1, k=5, seed=22))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vKEYg5D6I-lu",
        "outputId": "e97eee22-8f1e-4ed5-b0f5-4fb892329eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I recently ate a lot of spicy food and I'm still feeling it. I'm not sure if it's the heat or the fact that I'm still recovering from the spicy food.\\nI'm the same way. I'm not sure if\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High p and no top-k"
      ],
      "metadata": {
        "id": "V2b3HtmVI-uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopPSampler(p=0.8, k=None, seed=22))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cOwLlauzI-uc",
        "outputId": "8f38c5df-2d43-4f44-a8e3-5a2a7f41f164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate an entire bag of Oatmeal Raisin cookies in one sitting. So I guess I was very aware of the calorie count. The thing is, though, that they are delicious.\\nAs someone who eats an entire bag of Reese'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High p and top-k"
      ],
      "metadata": {
        "id": "DRyIF6VzI-2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_lm.compile(sampler=keras_nlp.samplers.TopPSampler(p=0.8, k=5, seed=22))\n",
        "opt_lm.generate(\"I recently ate\", max_length=50)"
      ],
      "metadata": {
        "id": "AFDnRt8tI-2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ce2a992f-a0f2-4187-9113-7f9482838b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently ate an ice cream sandwich. It was a good ice cream sandwich.\\nI recently ate an ice cream sandwich. It was a good ice cream sandwich.\\nI recently ate an ice cream sandwich. It was a good ice cream sandwich'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgeab-FgKKxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "https://keras.io/api/keras_nlp/samplers/"
      ],
      "metadata": {
        "id": "THLXKMoF62nW"
      }
    }
  ]
}